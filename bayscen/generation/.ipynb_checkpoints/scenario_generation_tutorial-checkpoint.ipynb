{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BayScen Scenario Generation - Tutorial\n",
    "\n",
    "This notebook demonstrates how to generate test scenarios using the BayScen framework.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The generation process:\n",
    "1. Load trained Bayesian Network model\n",
    "2. Generate scenarios using combinatorial coverage + conditional sampling\n",
    "3. Evaluate scenarios (realism, coverage, diversity)\n",
    "4. Export for testing\n",
    "\n",
    "**Scenarios:**\n",
    "- Scenario 1: Vehicle-Vehicle conflicts (8 environmental variables)\n",
    "- Scenario 2: Vehicle-Cyclist conflicts (9 environmental variables + Time_of_Day)\n",
    "\n",
    "**Modes:**\n",
    "- `rare`: Prioritize edge cases (recommended for testing)\n",
    "- `common`: Prioritize typical scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from generation.scenario_generator import BayesianScenarioGenerator\n",
    "from generation.evaluation_metrics import (\n",
    "    evaluate_scenarios,\n",
    "    compute_attribute_distributions,\n",
    "    compute_realism,\n",
    "    compute_coverage\n",
    ")\n",
    "from generation.generation_utils import (\n",
    "    validate_scenarios,\n",
    "    export_for_carla,\n",
    "    split_by_collision_point,\n",
    "    get_summary_statistics\n",
    ")\n",
    "from abstraction.abstract_variables import LEAF_NODES\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose scenario and mode\n",
    "SCENARIO = 2  # 1 or 2\n",
    "MODE = 'rare'  # 'common' or 'rare'\n",
    "\n",
    "# Define paths\n",
    "MODEL_PATH = f\"../modeling/models/scenario{SCENARIO}_full_bayesian_network.pkl\"\n",
    "DATA_PATH = \"../../data/processed/bayscen_final_data.csv\"  # For evaluation\n",
    "OUTPUT_DIR = Path(\"generated_scenarios\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Scenario: {SCENARIO}\")\n",
    "print(f\"  Mode: {MODE}\")\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Bayesian Network\n",
    "print(\"Loading Bayesian Network...\")\n",
    "with open(MODEL_PATH, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Model loaded successfully\")\n",
    "print(f\"  Total nodes: {len(model.nodes())}\")\n",
    "print(f\"  Total edges: {len(model.edges())}\")\n",
    "print(f\"\\n  Nodes: {sorted(model.nodes())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracted variables (from abstract_variables.py)\n",
    "abstracted_variables = LEAF_NODES\n",
    "print(\"Abstracted Variables:\")\n",
    "for var, values in abstracted_variables.items():\n",
    "    print(f\"  {var}: {values}\")\n",
    "\n",
    "# Concrete variables (scenario-specific)\n",
    "concrete_variables_s1 = [\n",
    "    \"Cloudiness\",\n",
    "    \"Wind_Intensity\",\n",
    "    \"Precipitation\",\n",
    "    \"Precipitation_Deposits\",\n",
    "    \"Wetness\",\n",
    "    \"Fog_Density\",\n",
    "    \"Road_Friction\",\n",
    "    \"Fog_Distance\",\n",
    "    \"Start_Ego\",\n",
    "    \"Goal_Ego\",\n",
    "    \"Start_Other\",\n",
    "    \"Goal_Other\"\n",
    "]\n",
    "\n",
    "concrete_variables_s2 = [\"Time_of_Day\"] + concrete_variables_s1\n",
    "\n",
    "concrete_variables = concrete_variables_s2 if SCENARIO == 2 else concrete_variables_s1\n",
    "\n",
    "print(f\"\\nConcrete Variables ({len(concrete_variables)}):\")\n",
    "print(f\"  {concrete_variables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "prefer_rare = (MODE == 'rare')\n",
    "\n",
    "generator = BayesianScenarioGenerator(\n",
    "    model=model,\n",
    "    leaf_nodes=abstracted_variables,\n",
    "    initial_nodes=concrete_variables,\n",
    "    similarity_threshold=0.1,\n",
    "    n_samples=100000,\n",
    "    use_sampling=True,\n",
    "    prefer_rare=prefer_rare\n",
    ")\n",
    "\n",
    "print(f\"✓ Generator created\")\n",
    "print(f\"  Mode: {MODE}\")\n",
    "print(f\"  Prefer rare: {prefer_rare}\")\n",
    "print(f\"  Similarity threshold: 0.1\")\n",
    "print(f\"  Number of samples: 100,000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Scenarios\n",
    "\n",
    "This will generate all combinations of abstracted variables and sample concrete parameters.\n",
    "\n",
    "**Expected output:** 648 scenarios (6×6×6×3 combinations)\n",
    "\n",
    "**Time:** ~10-15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "scenarios = generator.generate_scenarios()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Generation Summary:\")\n",
    "print(f\"  Total scenarios: {len(scenarios)}\")\n",
    "print(f\"  Columns: {list(scenarios.columns)}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inspect Generated Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few scenarios\n",
    "print(\"First 5 scenarios:\")\n",
    "scenarios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "summary = get_summary_statistics(scenarios)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by collision point\n",
    "print(\"\\nScenarios by Collision Point:\")\n",
    "print(scenarios['Collision_Point'].value_counts())\n",
    "\n",
    "# Split by collision point\n",
    "by_collision = split_by_collision_point(scenarios)\n",
    "for cp, df in by_collision.items():\n",
    "    print(f\"  {cp}: {len(df)} scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability distribution\n",
    "print(\"\\nProbability Distribution:\")\n",
    "print(f\"  Mean: {scenarios['probability'].mean():.6f}\")\n",
    "print(f\"  Median: {scenarios['probability'].median():.6f}\")\n",
    "print(f\"  Min: {scenarios['probability'].min():.6f}\")\n",
    "print(f\"  Max: {scenarios['probability'].max():.6f}\")\n",
    "\n",
    "# Plot histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(scenarios['probability'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Probability Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(scenarios['probability'], bins=50, edgecolor='black', log=True)\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.title('Probability Distribution (Log Scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Validate Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate scenarios\n",
    "validation = validate_scenarios(scenarios)\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "print(f\"  Valid: {validation['is_valid']}\")\n",
    "print(f\"  Total scenarios: {validation['num_scenarios']}\")\n",
    "\n",
    "if not validation['is_valid']:\n",
    "    print(f\"\\n  Issues found:\")\n",
    "    for issue in validation['issues']:\n",
    "        print(f\"    - {issue}\")\n",
    "else:\n",
    "    print(\"  ✓ All scenarios are valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Scenarios\n",
    "\n",
    "Compare generated scenarios against real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation attributes (environmental variables only)\n",
    "eval_attributes = [\n",
    "    \"Cloudiness\",\n",
    "    \"Wind_Intensity\",\n",
    "    \"Precipitation\",\n",
    "    \"Precipitation_Deposits\",\n",
    "    \"Wetness\",\n",
    "    \"Fog_Density\",\n",
    "    \"Road_Friction\",\n",
    "    \"Fog_Distance\"\n",
    "]\n",
    "\n",
    "if SCENARIO == 2:\n",
    "    eval_attributes = [\"Time_of_Day\"] + eval_attributes\n",
    "\n",
    "print(f\"Evaluation attributes: {eval_attributes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation\n",
    "results = evaluate_scenarios(\n",
    "    real_data_path=DATA_PATH,\n",
    "    generated_df=scenarios,\n",
    "    attributes=eval_attributes,\n",
    "    print_summary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realism metric\n",
    "print(f\"\\nRealism: {results['realism']:.1f}%\")\n",
    "print(f\"  → {results['realism']:.1f}% of scenarios are within real-world distribution\")\n",
    "\n",
    "if results['realism'] > 90:\n",
    "    print(\"  ✓ Excellent realism!\")\n",
    "elif results['realism'] > 70:\n",
    "    print(\"  ✓ Good realism\")\n",
    "else:\n",
    "    print(\"  ⚠ Consider adjusting generation parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage metric\n",
    "coverage = results['coverage']\n",
    "print(f\"\\nCoverage: {coverage['coverage_percentage']:.1f}%\")\n",
    "print(f\"  Covered: {coverage['num_covered']}/{coverage['total_real_unique']} unique real scenarios\")\n",
    "\n",
    "if coverage['coverage_percentage'] > 90:\n",
    "    print(\"  ✓ Excellent coverage!\")\n",
    "elif coverage['coverage_percentage'] > 70:\n",
    "    print(\"  ✓ Good coverage\")\n",
    "else:\n",
    "    print(\"  ⚠ Consider increasing scenario diversity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversity metric\n",
    "unique = results['num_unique']\n",
    "uniqueness = (unique / len(scenarios)) * 100\n",
    "\n",
    "print(f\"\\nDiversity:\")\n",
    "print(f\"  Unique scenarios: {unique}\")\n",
    "print(f\"  Total scenarios: {len(scenarios)}\")\n",
    "print(f\"  Uniqueness: {uniqueness:.1f}%\")\n",
    "\n",
    "if uniqueness > 95:\n",
    "    print(\"  ✓ Excellent diversity!\")\n",
    "elif uniqueness > 80:\n",
    "    print(\"  ✓ Good diversity\")\n",
    "else:\n",
    "    print(\"  ⚠ Some duplicate scenarios detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions for specific attributes\n",
    "from generation.evaluation_metrics import compare_distributions\n",
    "\n",
    "# Example: Compare Cloudiness distribution\n",
    "compare_distributions(\n",
    "    results['distributions'],\n",
    "    'Cloudiness',\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all distributions\n",
    "for attr in eval_attributes[:3]:  # Show first 3\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    compare_distributions(\n",
    "        results['distributions'],\n",
    "        attr,\n",
    "        plot=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "output_csv = OUTPUT_DIR / f\"scenario{SCENARIO}_{MODE}_scenarios.csv\"\n",
    "scenarios.to_csv(output_csv, index=False)\n",
    "print(f\"✓ Saved scenarios to: {output_csv}\")\n",
    "\n",
    "# Save to Excel\n",
    "output_excel = OUTPUT_DIR / f\"scenario{SCENARIO}_{MODE}_scenarios.xlsx\"\n",
    "generator.save_scenarios(scenarios, str(output_excel))\n",
    "print(f\"✓ Saved scenarios to: {output_excel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "eval_output = OUTPUT_DIR / f\"scenario{SCENARIO}_{MODE}_evaluation.pkl\"\n",
    "with open(eval_output, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(f\"✓ Saved evaluation results to: {eval_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Export for CARLA (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for CARLA simulator\n",
    "carla_output = OUTPUT_DIR / f\"scenario{SCENARIO}_{MODE}_carla.csv\"\n",
    "export_for_carla(scenarios, str(carla_output))\n",
    "print(f\"✓ Exported CARLA-compatible scenarios to: {carla_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Common vs Rare Modes (Optional)\n",
    "\n",
    "If you've generated both modes, you can compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both modes for comparison\n",
    "try:\n",
    "    common_scenarios = pd.read_csv(OUTPUT_DIR / f\"scenario{SCENARIO}_common_scenarios.csv\")\n",
    "    rare_scenarios = pd.read_csv(OUTPUT_DIR / f\"scenario{SCENARIO}_rare_scenarios.csv\")\n",
    "    \n",
    "    print(\"Comparing Common vs Rare Modes:\\n\")\n",
    "    \n",
    "    print(f\"Common Scenarios:\")\n",
    "    print(f\"  Count: {len(common_scenarios)}\")\n",
    "    print(f\"  Mean probability: {common_scenarios['probability'].mean():.6f}\")\n",
    "    print(f\"  Median probability: {common_scenarios['probability'].median():.6f}\")\n",
    "    \n",
    "    print(f\"\\nRare Scenarios:\")\n",
    "    print(f\"  Count: {len(rare_scenarios)}\")\n",
    "    print(f\"  Mean probability: {rare_scenarios['probability'].mean():.6f}\")\n",
    "    print(f\"  Median probability: {rare_scenarios['probability'].median():.6f}\")\n",
    "    \n",
    "    # Compare using utility function\n",
    "    from generation.generation_utils import compare_scenario_sets\n",
    "    compare_scenario_sets(\n",
    "        common_scenarios,\n",
    "        rare_scenarios,\n",
    "        eval_attributes,\n",
    "        \"Common\",\n",
    "        \"Rare\"\n",
    "    )\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Both common and rare scenarios needed for comparison.\")\n",
    "    print(\"Generate both modes to compare:\")\n",
    "    print(\"  MODE = 'common'  # Run once\")\n",
    "    print(\"  MODE = 'rare'    # Run again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✓ Loading trained Bayesian Network\n",
    "2. ✓ Configuring scenario generator\n",
    "3. ✓ Generating test scenarios\n",
    "4. ✓ Validating scenario quality\n",
    "5. ✓ Evaluating realism, coverage, and diversity\n",
    "6. ✓ Exporting scenarios for testing\n",
    "\n",
    "**Next Steps:**\n",
    "- Run scenarios in CARLA simulator\n",
    "- Analyze test results\n",
    "- Compare with baseline methods (Random, SitCov, CTBC, PICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "### Generate Both Scenarios\n",
    "\n",
    "```python\n",
    "# Scenario 1 - Rare\n",
    "SCENARIO = 1\n",
    "MODE = 'rare'\n",
    "# Run cells above\n",
    "\n",
    "# Scenario 1 - Common\n",
    "SCENARIO = 1\n",
    "MODE = 'common'\n",
    "# Run cells above\n",
    "\n",
    "# Scenario 2 - Rare\n",
    "SCENARIO = 2\n",
    "MODE = 'rare'\n",
    "# Run cells above\n",
    "\n",
    "# Scenario 2 - Common\n",
    "SCENARIO = 2\n",
    "MODE = 'common'\n",
    "# Run cells above\n",
    "```\n",
    "\n",
    "### Command Line Alternative\n",
    "\n",
    "```bash\n",
    "cd bayscen/generation\n",
    "\n",
    "# Generate all variants\n",
    "python generate_scenarios.py --scenario 1 --mode rare\n",
    "python generate_scenarios.py --scenario 1 --mode common\n",
    "python generate_scenarios.py --scenario 2 --mode rare\n",
    "python generate_scenarios.py --scenario 2 --mode common\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
